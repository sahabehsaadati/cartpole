# -*- coding: utf-8 -*-
"""Deep Q-Learning cartpole.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XWkW6dOOT4TUpRvta26qBIMpcqRer5XE
"""

# Current stable release of DI-engine
!pip install --upgrade pip
!pip install git+https://github.com/opendilab/DI-engine.git@main#egg=DI-engine

!python -m pip install --upgrade pip

pip install --upgrade pip

pip install --upgrade pip

pip install requests -i https://mirrors.aliyun.com/pypi/simple/ DI-engine

!conda install -c opendilab di-engine

pip install gitpython

pip install requests -i https://mirrors.aliyun.com/pypi/simple/ DI-engine

!conda install -c opendilab di-engine

!git clone https://github.com/opendilab/DI-engine.git

cd DI-engine

pip install .

!git clone https://github.com/opendilab/DI-engine.git

pip install . --user

pip install --upgrade pip

pip install box2d-py

!sudo apt-get install build-essential python-dev swig python-pygame

pip install DI-engine[common_env]

pip install DI-engine[test]

pip install DI-engine[fast]

pip install DI-engine[common_env,test,fast]

import ding
print(ding.__version__)

!ding -v

from dizoo.classic_control.cartpole.config.cartpole_dqn_config import main_config, create_config
from ding.config import compile_config

cfg = compile_config(main_config, create_cfg=create_config, auto=True)

import gym
from ding.envs import DingEnvWrapper, BaseEnvManagerV2

collector_env = BaseEnvManagerV2(
    env_fn=[lambda: DingEnvWrapper(gym.make("CartPole-v0")) for _ in range(cfg.env.collector_env_num)],
    cfg=cfg.env.manager
)
evaluator_env = BaseEnvManagerV2(
    env_fn=[lambda: DingEnvWrapper(gym.make("CartPole-v0")) for _ in range(cfg.env.evaluator_env_num)],
    cfg=cfg.env.manager
)

from ding.model import DQN
from ding.policy import DQNPolicy
from ding.data import DequeBuffer

model = DQN(**cfg.policy.model)
buffer_ = DequeBuffer(size=cfg.policy.other.replay_buffer.replay_buffer_size)
policy = DQNPolicy(cfg.policy, model=model)

!import logging
logging.getLogger().setLevel(logging.INFO)

from ding.framework import task
from ding.framework.context import OnlineRLContext
from ding.framework.middleware import OffPolicyLearner, StepCollector, interaction_evaluator, data_pusher, eps_greedy_handler, CkptSaver

with task.start(async_mode=False, ctx=OnlineRLContext()):
    # Evaluating, we place it on the first place to get the score of the random model as a benchmark value
    task.use(interaction_evaluator(cfg, policy.eval_mode, evaluator_env))
    task.use(eps_greedy_handler(cfg))  # Decay probability of explore-exploit
    task.use(StepCollector(cfg, policy.collect_mode, collector_env))  # Collect environmental data
    task.use(data_pusher(cfg, buffer_))  # Push data to buffer
    task.use(OffPolicyLearner(cfg, policy.learn_mode, buffer_))  # Train the model
    task.use(CkptSaver(cfg, policy, train_freq=100))  # Save the model
    # In the evaluation process, if the model is found to have exceeded the convergence value, it will end early here
    task.run()